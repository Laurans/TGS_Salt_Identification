{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import warnings\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-white')\n",
    "import seaborn as sns\n",
    "sns.set_style(\"white\")\n",
    "\n",
    "import cv2\n",
    "\n",
    "from tqdm import tqdm_notebook, tnrange, tqdm\n",
    "from itertools import chain\n",
    "from skimage.io import imread, imshow, concatenate_images\n",
    "from skimage.transform import resize\n",
    "from skimage.morphology import label\n",
    "\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, Dropout\n",
    "from keras.layers.core import Lambda\n",
    "from keras.layers.convolutional import Conv2D, Conv2DTranspose\n",
    "from keras.layers.pooling import MaxPooling2D\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras import backend as K\n",
    "\n",
    "from scipy import misc, ndimage\n",
    "from scipy.ndimage.interpolation import map_coordinates\n",
    "from scipy.ndimage.filters import gaussian_filter\n",
    "from skimage.morphology import reconstruction, disk\n",
    "from skimage.filters import rank\n",
    "from skimage import img_as_float, exposure\n",
    "\n",
    "import tensorflow as tf\n",
    "from imgaug import augmenters as iaa\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Set some parameters\n",
    "im_width = 128\n",
    "im_height = 128\n",
    "im_chan = 1\n",
    "img_size_ori = 101\n",
    "\n",
    "path_train = '../data/train/'\n",
    "path_test = '../data/test/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ids = next(os.walk(path_train+\"images\"))[2]\n",
    "test_ids = next(os.walk(path_test+'images'))[2]\n",
    "\n",
    "def cov_to_class(val):    \n",
    "    for i in range(0, 11):\n",
    "        if val * 10 <= i :\n",
    "            return i\n",
    "\n",
    "# Get and resize train images and masks\n",
    "X_train = np.zeros((len(train_ids), im_height, im_width, im_chan), dtype=np.uint8)\n",
    "coverage = np.zeros((len(train_ids), 2), dtype=np.float32)\n",
    "Y_train = np.zeros((len(train_ids), im_height, im_width, 1), dtype=np.uint8)\n",
    "\n",
    "print('Getting and resizing train images and mask ...')\n",
    "sys.stdout.flush()\n",
    "\n",
    "for n, id_ in enumerate(tqdm(train_ids)):\n",
    "    path = path_train\n",
    "    img = load_img(path + '/images/' + id_, color_mode = \"grayscale\")\n",
    "    x = img_to_array(img)\n",
    "    x = resize(x, (128, 128, 1), mode='constant', preserve_range=True)\n",
    "    X_train[n] = x\n",
    "    \n",
    "    mask = img_to_array(load_img(path + '/masks/' + id_, color_mode = \"grayscale\"))\n",
    "    Y_train[n] = resize(mask, (128, 128, 1), mode='constant', preserve_range=True)\n",
    "    \n",
    "    coverage[n, 0] = (mask / 255).sum() / img_size_ori**2\n",
    "    coverage[n, 1] = cov_to_class(coverage[n, 0])\n",
    "    \n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build U-Net Model\n",
    "def create_model():\n",
    "    inputs = Input((im_height, im_width, im_chan))\n",
    "    s = Lambda(lambda x: x/255)(inputs)\n",
    "\n",
    "    c1 = Conv2D(8, (3, 3), activation='relu', padding='same') (s)\n",
    "    c1 = Conv2D(8, (3, 3), activation='relu', padding='same') (c1)\n",
    "    p1 = MaxPooling2D((2, 2)) (c1)\n",
    "    p1 = Dropout(0.25)(p1)\n",
    "\n",
    "    c2 = Conv2D(16, (3, 3), activation='relu', padding='same') (p1)\n",
    "    c2 = Conv2D(16, (3, 3), activation='relu', padding='same') (c2)\n",
    "    p2 = MaxPooling2D((2, 2)) (c2)\n",
    "    p2 = Dropout(0.5)(p2)\n",
    "\n",
    "    c3 = Conv2D(32, (3, 3), activation='relu', padding='same') (p2)\n",
    "    c3 = Conv2D(32, (3, 3), activation='relu', padding='same') (c3)\n",
    "    p3 = MaxPooling2D((2, 2)) (c3)\n",
    "    p3 = Dropout(0.5)(p3)\n",
    "\n",
    "    c4 = Conv2D(64, (3, 3), activation='relu', padding='same') (p3)\n",
    "    c4 = Conv2D(64, (3, 3), activation='relu', padding='same') (c4)\n",
    "    p4 = MaxPooling2D(pool_size=(2, 2)) (c4)\n",
    "    p4 = Dropout(0.5)(p4)\n",
    "\n",
    "    c5 = Conv2D(128, (3, 3), activation='relu', padding='same') (p4)\n",
    "    c5 = Conv2D(128, (3, 3), activation='relu', padding='same') (c5)\n",
    "\n",
    "    u6 = Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same') (c5)\n",
    "    u6 = concatenate([u6, c4])\n",
    "    u6 = Dropout(0.5)(u6)\n",
    "    c6 = Conv2D(64, (3, 3), activation='relu', padding='same') (u6)\n",
    "    c6 = Conv2D(64, (3, 3), activation='relu', padding='same') (c6)\n",
    "\n",
    "    u7 = Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same') (c6)\n",
    "    u7 = concatenate([u7, c3])\n",
    "    u7 = Dropout(0.5)(u7)\n",
    "    c7 = Conv2D(32, (3, 3), activation='relu', padding='same') (u7)\n",
    "    c7 = Conv2D(32, (3, 3), activation='relu', padding='same') (c7)\n",
    "\n",
    "    u8 = Conv2DTranspose(16, (2, 2), strides=(2, 2), padding='same') (c7)\n",
    "    u8 = concatenate([u8, c2])\n",
    "    u8 = Dropout(0.5)(u8)\n",
    "    c8 = Conv2D(16, (3, 3), activation='relu', padding='same') (u8)\n",
    "    c8 = Conv2D(16, (3, 3), activation='relu', padding='same') (c8)\n",
    "\n",
    "    u9 = Conv2DTranspose(8, (2, 2), strides=(2, 2), padding='same') (c8)\n",
    "    u9 = concatenate([u9, c1], axis=3)\n",
    "    u9 = Dropout(0.5)(u9)\n",
    "    c9 = Conv2D(8, (3, 3), activation='relu', padding='same') (u9)\n",
    "    c9 = Conv2D(8, (3, 3), activation='relu', padding='same') (c9)\n",
    "\n",
    "    outputs = Conv2D(1, (1, 1), activation='sigmoid') (c9)\n",
    "\n",
    "    model = Model(inputs=[inputs], outputs=[outputs])\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def fit(model, X_train, Y_train, output_name):\n",
    "    earlystopper = EarlyStopping(patience=10, verbose=0)\n",
    "    checkpointer = ModelCheckpoint('{}.h5'.format(output_name), verbose=0, save_best_only=True)\n",
    "    reduce_lr = ReduceLROnPlateau(factor=0.1, patience=5, min_lr=0.00001, verbose=0)\n",
    "    \n",
    "    results = model.fit(X_train, Y_train, validation_data=[x_valid, y_valid], batch_size=128, epochs=100, \n",
    "                        callbacks=[earlystopper, checkpointer, reduce_lr], verbose=0)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create train/validation split stratified by salt coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def downsample(img):\n",
    "    return resize(img, (img_size_ori, img_size_ori), mode='constant', preserve_range=True)\n",
    "\n",
    "def filtering_regional_maxima(img):\n",
    "    image = img_as_float(img)\n",
    "    image = ndimage.gaussian_filter(image, 1)\n",
    "    \n",
    "    seed = np.copy(image)\n",
    "    seed[1:-1, 1:-1] = image.min()\n",
    "    mask = image\n",
    "    dilated = reconstruction(seed, mask, method='dilation') * 255\n",
    "    return np.array(dilated, dtype=np.uint8)\n",
    "\n",
    "def global_equalize(img):\n",
    "    return np.array(exposure.equalize_hist(img) * 255, dtype=np.uint8)\n",
    "\n",
    "def elastic_transform(image, alpha, sigma, seed=None):\n",
    "    \"\"\"Elastic deformation of images as described in [Simard2003]_.\n",
    "    .. [Simard2003] Simard, Steinkraus and Platt, \"Best Practices for\n",
    "       Convolutional Neural Networks applied to Visual Document Analysis\", in\n",
    "       Proc. of the International Conference on Document Analysis and\n",
    "       Recognition, 2003.\n",
    "    \"\"\"\n",
    "    \n",
    "    if seed is None:\n",
    "        random_state = np.random.RandomState()\n",
    "    else:\n",
    "        random_state = np.random.RandomState(seed=seed)\n",
    "    \n",
    "    shape = image.shape\n",
    "    dx = gaussian_filter((random_state.rand(*shape) * 2 - 1), sigma, mode=\"constant\", cval=0) * alpha\n",
    "    dy = gaussian_filter((random_state.rand(*shape) * 2 - 1), sigma, mode=\"constant\", cval=0) * alpha\n",
    "\n",
    "    x, y = np.meshgrid(np.arange(shape[0]), np.arange(shape[1]))\n",
    "    indices = np.reshape(y+dy, (-1, 1)), np.reshape(x+dx, (-1, 1))\n",
    "\n",
    "    return map_coordinates(image, indices, order=1).reshape(shape)\n",
    "\n",
    "\n",
    "def iou_metric(y_true_in, y_pred_in, print_table=False):\n",
    "    labels = y_true_in\n",
    "    y_pred = y_pred_in\n",
    "    \n",
    "    true_objects = 2\n",
    "    pred_objects = 2\n",
    "\n",
    "    intersection = np.histogram2d(labels.flatten(), y_pred.flatten(), bins=(true_objects, pred_objects))[0]\n",
    "\n",
    "    # Compute areas (needed for finding the union between all objects)\n",
    "    area_true = np.histogram(labels, bins = true_objects)[0]\n",
    "    area_pred = np.histogram(y_pred, bins = pred_objects)[0]\n",
    "    area_true = np.expand_dims(area_true, -1)\n",
    "    area_pred = np.expand_dims(area_pred, 0)\n",
    "\n",
    "    # Compute union\n",
    "    union = area_true + area_pred - intersection\n",
    "\n",
    "    # Exclude background from the analysis\n",
    "    intersection = intersection[1:,1:]\n",
    "    union = union[1:,1:]\n",
    "    union[union == 0] = 1e-9\n",
    "\n",
    "    # Compute the intersection over union\n",
    "    iou = intersection / union\n",
    "\n",
    "    # Precision helper function\n",
    "    def precision_at(threshold, iou):\n",
    "        matches = iou > threshold\n",
    "        true_positives = np.sum(matches, axis=1) == 1   # Correct objects\n",
    "        false_positives = np.sum(matches, axis=0) == 0  # Missed objects\n",
    "        false_negatives = np.sum(matches, axis=1) == 0  # Extra objects\n",
    "        tp, fp, fn = np.sum(true_positives), np.sum(false_positives), np.sum(false_negatives)\n",
    "        return tp, fp, fn\n",
    "\n",
    "    # Loop over IoU thresholds\n",
    "    prec = []\n",
    "    if print_table:\n",
    "        print(\"Thresh\\tTP\\tFP\\tFN\\tPrec.\")\n",
    "    for t in np.arange(0.5, 1.0, 0.05):\n",
    "        tp, fp, fn = precision_at(t, iou)\n",
    "        if (tp + fp + fn) > 0:\n",
    "            p = tp / (tp + fp + fn)\n",
    "        else:\n",
    "            p = 0\n",
    "        if print_table:\n",
    "            print(\"{:1.3f}\\t{}\\t{}\\t{}\\t{:1.3f}\".format(t, tp, fp, fn, p))\n",
    "        prec.append(p)\n",
    "    \n",
    "    if print_table:\n",
    "        print(\"AP\\t-\\t-\\t-\\t{:1.3f}\".format(np.mean(prec)))\n",
    "    return np.mean(prec)\n",
    "\n",
    "def iou_metric_batch(y_true_in, y_pred_in):\n",
    "    batch_size = y_true_in.shape[0]\n",
    "    metric = []\n",
    "    for batch in range(batch_size):\n",
    "        value = iou_metric(y_true_in[batch], y_pred_in[batch])\n",
    "        metric.append(value)\n",
    "    return np.mean(metric)\n",
    "\n",
    "def augment_images(x_train, y_train):\n",
    "    x_train_ = np.append(x_train, np.array( [filtering_regional_maxima(x) for x in tqdm(x_train)]), 0)\n",
    "    y_train_ = np.vstack([y_train, y_train.copy()])\n",
    "    print('Filtering done')\n",
    "\n",
    "    x_train_ = np.append(x_train_, np.array([global_equalize(x) for x in tqdm(x_train)]), 0)\n",
    "    y_train_ = np.vstack([y_train_, y_train.copy()])\n",
    "    print('Global equalize done')\n",
    "\n",
    "    x_train_ = np.append(x_train_, np.array([np.expand_dims(elastic_transform(x.squeeze(), 20, 4, 20), -1) \n",
    "                                  for x in tqdm(x_train)]), 0)\n",
    "    y_train_ = np.append(y_train_, np.array([np.expand_dims(elastic_transform(x.squeeze(), 20, 4, 20), -1) \n",
    "                                  for x in tqdm(y_train)]), 0)\n",
    "\n",
    "    print('Elastic transform done')\n",
    "    return x_train_, y_train_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ious_by_split = []\n",
    "thresholds_by_split = []\n",
    "\n",
    "\n",
    "for i in range(3):\n",
    "    print('-'*5,'Start FOLD', '-'*5)\n",
    "    x_train, x_valid, y_train, y_valid, cov_train, cov_valid = train_test_split(\n",
    "        X_train, Y_train, coverage, test_size=0.1, stratify=coverage[:, 1], random_state=1337)\n",
    "    \n",
    "    ## Data augmentation\n",
    "    x_train = np.append(x_train, np.array( [np.fliplr(x) for x in x_train]), 0)\n",
    "    y_train = np.append(y_train, np.array( [np.fliplr(x) for x in y_train]), 0)\n",
    "    print('Flip left right done')\n",
    "    print(x_train.shape)\n",
    "\n",
    "    x_train_, y_train_ = augment_images(x_train, y_train)\n",
    "    \n",
    "    y_train_ = np.piecewise(y_train_, [y_train_ > 125, y_train_ < 125], [1, 0])\n",
    "\n",
    "    y_valid = np.piecewise(y_valid, [y_valid > 125, y_valid < 125], [1, 0])\n",
    "\n",
    "    ## Create model\n",
    "    amodel = create_model()\n",
    "    history = fit(amodel, x_train_, y_train_, 'model_split_{}'.format(i))\n",
    "\n",
    "    model = load_model('model_split_{}.h5'.format(i))\n",
    "\n",
    "    preds_valid = model.predict(x_valid).reshape(-1, im_height, im_width)\n",
    "\n",
    "    thresholds = np.linspace(0, 1, 50)\n",
    "    ious = np.array([iou_metric_batch(y_valid, np.int32(preds_valid > threshold)) for threshold in tqdm(thresholds)])\n",
    "\n",
    "    threshold_best_index = np.argmax(ious[9:-10]) + 9\n",
    "    iou_best = ious[threshold_best_index]\n",
    "    threshold_best = thresholds[threshold_best_index]\n",
    "    \n",
    "    ious_by_split.append(iou_best)\n",
    "    thresholds_by_split.append(threshold_best)\n",
    "    \n",
    "pickle.dump([ious_by_split, thresholds_by_split], open('ious_thres.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best threshold and ious"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Threshold \", np.mean(thresholds_by_split), 'Ious', np.mean(ious_by_split))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrain model on all data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict X_test and create submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.zeros((len(test_ids), im_height, im_width, im_chan), dtype=np.uint8)\n",
    "\n",
    "sizes_test = []\n",
    "print('Getting and resizing test images ... ')\n",
    "sys.stdout.flush()\n",
    "for n, id_ in enumerate(tqdm(test_ids)):\n",
    "    path = path_test\n",
    "    img = load_img(path + '/images/' + id_)\n",
    "    x = img_to_array(img)[:,:,1]\n",
    "    sizes_test.append([x.shape[0], x.shape[1]])\n",
    "    x = resize(x, (128, 128, 1), mode='constant', preserve_range=True)\n",
    "    X_test[n] = x\n",
    "\n",
    "print('Done!')\n",
    "\n",
    "preds_test = (model.predict(X_test, verbose=1) > threshold_best).astype(np.uint8)\n",
    "\n",
    "# Create list of upsampled test masks\n",
    "preds_test_upsampled = [np.squeeze(downsample(x)) for x in preds_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RLenc(img, order='F', format=True):\n",
    "    \"\"\"\n",
    "    img is binary mask image, shape (r,c)\n",
    "    order is down-then-right, i.e. Fortran\n",
    "    format determines if the order needs to be preformatted (according to submission rules) or not\n",
    "\n",
    "    returns run length as an array or string (if format is True)\n",
    "    \"\"\"\n",
    "    bytes = img.reshape(img.shape[0] * img.shape[1], order=order)\n",
    "    runs = []  ## list of run lengths\n",
    "    r = 0  ## the current run length\n",
    "    pos = 1  ## count starts from 1 per WK\n",
    "    for c in bytes:\n",
    "        if (c == 0):\n",
    "            if r != 0:\n",
    "                runs.append((pos, r))\n",
    "                pos += r\n",
    "                r = 0\n",
    "            pos += 1\n",
    "        else:\n",
    "            r += 1\n",
    "\n",
    "    # if last run is unsaved (i.e. data ends with 1)\n",
    "    if r != 0:\n",
    "        runs.append((pos, r))\n",
    "        pos += r\n",
    "        r = 0\n",
    "\n",
    "    if format:\n",
    "        z = ''\n",
    "\n",
    "        for rr in runs:\n",
    "            z += '{} {} '.format(rr[0], rr[1])\n",
    "        return z[:-1]\n",
    "    else:\n",
    "        return runs\n",
    "\n",
    "pred_dict = {fn[:-4]:RLenc(np.round(preds_test_upsampled[i])) for i,fn in enumerate(tqdm(test_ids))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.DataFrame.from_dict(pred_dict,orient='index')\n",
    "sub.index.names = ['id']\n",
    "sub.columns = ['rle_mask']\n",
    "sub.to_csv('submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
