def ResNet152(img_shape, start_ch=64, depth=4, inc_rate=2):
    def _identity_block(input_tensor, kernel_size, filters, stage, block):
        """The identity_block is the block that has no conv layer at shortcut

        Keyword arguments
        input_tensor -- input tensor
        kernel_size -- defualt 3, the kernel size of middle conv layer at main path
        filters -- list of integers, the nb_filters of 3 conv layer at main path
        stage -- integer, current stage label, used for generating layer names
        block -- 'a','b'..., current block label, used for generating layer names

        """
        eps = 1.1e-5
        bn_axis = 3

        nb_filter1, nb_filter2, nb_filter3 = filters
        conv_name_base = 'res' + str(stage) + block + '_branch'
        bn_name_base = 'bn' + str(stage) + block + '_branch'
        scale_name_base = 'scale' + str(stage) + block + '_branch'

        x = Conv2D(nb_filter1, (1, 1), name=conv_name_base + '2a', use_bias=False)(input_tensor)
        x = BatchNormalization(epsilon=eps, axis=bn_axis, name=bn_name_base + '2a')(x)
        x = Scale(axis=bn_axis, name=scale_name_base + '2a')(x)
        x = Activation('relu', name=conv_name_base + '2a_relu')(x)

        x = ZeroPadding2D((1, 1), name=conv_name_base + '2b_zeropadding')(x)
        x = Conv2D(nb_filter2, (kernel_size, kernel_size), name=conv_name_base + '2b', use_bias=False)(x)
        x = BatchNormalization(epsilon=eps, axis=bn_axis, name=bn_name_base + '2b')(x)
        x = Scale(axis=bn_axis, name=scale_name_base + '2b')(x)
        x = Activation('relu', name=conv_name_base + '2b_relu')(x)

        x = Conv2D(nb_filter3, (1, 1), name=conv_name_base + '2c', use_bias=False)(x)
        x = BatchNormalization(epsilon=eps, axis=bn_axis, name=bn_name_base + '2c')(x)
        x = Scale(axis=bn_axis, name=scale_name_base + '2c')(x)

        x = add([x, input_tensor], name='res' + str(stage) + block)
        x = Activation('relu', name='res' + str(stage) + block + '_relu')(x)
        return x

    def _conv_block(input_tensor, kernel_size, filters, stage, block, strides=(2,2)):
        eps = 1.1e-5
        bn_axis = 3

        nb_filter1, nb_filter2, nb_filter3 = filters
        conv_name_base = 'res'+str(stage) + block + '_branch'
        bn_name_base = 'bn'+str(stage)+ block + '_branch'
        scale_name_base = 'scale' + str(stage) + block + '_branch'

        x = Conv2D(nb_filter1, (1, 1), strides=strides, name=conv_name_base+'2a', use_bias=False)(input_tensor)
        x = BatchNormalization(epsilon=eps, axis=bn_axis, name=bn_name_base + '2a')(x)
        x = Scale(axis=bn_axis, name=scale_name_base + '2a')(x)
        x = Activation('relu', name=conv_name_base + '2a_relu')(x)

        x = ZeroPadding2D((1, 1), name=conv_name_base + '2b_zeropadding')(x)
        x = Conv2D(nb_filter2, (kernel_size, kernel_size), name=conv_name_base + '2b', use_bias=False)(x)
        x = BatchNormalization(epsilon=eps, axis=bn_axis, name=bn_name_base + '2b')(x)
        x = Scale(axis=bn_axis, name=scale_name_base + '2b')(x)
        x = Activation('relu', name=conv_name_base + '2b_relu')(x)
        
        x = Conv2D(nb_filter3, (1, 1), name=conv_name_base + '2c', use_bias=False)(x)
        x = BatchNormalization(epsilon=eps, axis=bn_axis, name=bn_name_base + '2c')(x)
        x = Scale(axis=bn_axis, name=scale_name_base + '2c')(x)

        shortcut = Conv2D(nb_filter3, (1, 1), strides=strides, name=conv_name_base + '1', use_bias=False)(input_tensor)
        shortcut = BatchNormalization(epsilon=eps, axis=bn_axis, name=bn_name_base + '1')(shortcut)
        shortcut = Scale(axis=bn_axis, name=scale_name_base + '1')(shortcut)

        x = add([x, shortcut], name='res' + str(stage) + block)
        x = Activation('relu', name='res' + str(stage) + block + '_relu')(x)
        return x

    def _level_block(input_tensor, filters, depth, inc):
        if depth > 0:
            n = _conv_block(input_tensor, 3, filters, stage=depth, block='a', strides=(1,1))
            print('depth', depth, 'n -->', n.shape)
            m = MaxPooling2D()(n)

            m = _level_block(m, list(map(lambda x: x*inc, filters)), depth-1, inc)

            k = Conv2DTranspose(filters[-1], 3, strides=2, activation='relu', padding='same')(m)
            print('depth', depth, 'n -->', n.shape)
            print('depth', depth, 'k -->', k.shape)
            m = Concatenate()([n, k])
        else:
            m = _conv_block(input_tensor, 3, filters, stage=depth, block='a', strides=(1,1))
        print('depth', depth, 'm -->', m.shape)
        return m
    eps = 1.1e-5
    bn_axis = 3
    inputs = Input(shape=img_shape)
    x = ZeroPadding2D((2,2), name='conv1_zeropadding')(inputs)
    x = Conv2D(start_ch, (3, 3), strides=(1,1), padding='same', name='conv1', use_bias=False)(x)
    x = BatchNormalization(epsilon=eps, axis=bn_axis, name='bn_conv1')(x)
    x = Scale(axis=bn_axis, name='scale_conv1')(x)
    x = Activation('relu', name='conv1_relu')(x)
    x = _level_block(x, [start_ch, start_ch, start_ch*4], depth, inc_rate)
    o = Cropping2D(cropping=(2,2))(x)
    o = Conv2D(1, 1, activation='sigmoid')(o)

    return Model(inputs=inputs, outputs=o)